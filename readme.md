Maze Solver via Markov Decision Processes
Overview

This project implements a deterministic Markov Decision Process (MDP) and solves it using the Value Iteration algorithm to obtain an optimal navigation policy for 2D mazes.
Each maze cell is treated as a state, and actions correspond to four directional movements.
The implementation demonstrates the convergence of the Bellman optimality updates and the scalability of dynamic programming in reinforcement learning.

Formulation

An MDP is defined as:

ğ‘€
=
âŸ¨
ğ‘†
,
ğ´
,
ğ‘ƒ
,
ğ‘…
,
ğ›¾
âŸ©
M=âŸ¨S,A,P,R,Î³âŸ©

where

ğ‘†
S: set of valid grid states

ğ´
=
{
up
,
down
,
left
,
right
}
A={up,down,left,right}: action space

ğ‘ƒ
(
ğ‘ 
â€²
âˆ£
ğ‘ 
,
ğ‘
)
P(s
â€²
âˆ£s,a): deterministic transition model

ğ‘…
(
ğ‘ 
,
ğ‘
,
ğ‘ 
â€²
)
R(s,a,s
â€²
): reward function

ğ›¾
âˆˆ
(
0
,
1
)
Î³âˆˆ(0,1): discount factor

The optimal value function satisfies the Bellman equation:

ğ‘‰
âˆ—
(
ğ‘ 
)
=
max
â¡
ğ‘
[
ğ‘…
(
ğ‘ 
,
ğ‘
)
+
ğ›¾
ğ‘‰
âˆ—
(
ğ‘ 
â€²
)
]
V
âˆ—
(s)=
a
max
	â€‹

[R(s,a)+Î³V
âˆ—
(s
â€²
)]
Algorithm

Value Iteration Update:

ğ‘‰
ğ‘˜
+
1
(
ğ‘ 
)
=
max
â¡
ğ‘
[
ğ‘…
(
ğ‘ 
,
ğ‘
)
+
ğ›¾
ğ‘‰
ğ‘˜
(
ğ‘ 
â€²
)
]
V
k+1
	â€‹

(s)=
a
max
	â€‹

[R(s,a)+Î³V
k
	â€‹

(s
â€²
)]

Convergence Criterion:

âˆ£
ğ‘‰
ğ‘˜
+
1
(
ğ‘ 
)
âˆ’
ğ‘‰
ğ‘˜
(
ğ‘ 
)
âˆ£
<
ğœ€
âˆ£V
k+1
	â€‹

(s)âˆ’V
k
	â€‹

(s)âˆ£<Îµ

Optimal Policy Extraction:

ğœ‹
âˆ—
(
ğ‘ 
)
=
arg
â¡
max
â¡
ğ‘
[
ğ‘…
(
ğ‘ 
,
ğ‘
)
+
ğ›¾
ğ‘‰
âˆ—
(
ğ‘ 
â€²
)
]
Ï€
âˆ—
(s)=arg
a
max
	â€‹

[R(s,a)+Î³V
âˆ—
(s
â€²
)]

Computational Complexity:

ğ‘‚
(
ğ¾
âˆ£
ğ‘†
âˆ£
âˆ£
ğ´
âˆ£
)
O(Kâˆ£Sâˆ£âˆ£Aâˆ£)

where 
ğ¾
K is the number of iterations until convergence.

Reward Function
ğ‘…
(
ğ‘ 
,
ğ‘
,
ğ‘ 
â€²
)
=
{
+
100
,
	
ğ‘ 
â€²
=
ğ‘ 
goal


âˆ’
5
,
	
ğ‘ 
â€²
=
ğ‘ 
Â (invalidÂ move)


âˆ’
0.1
,
	
otherwise
R(s,a,s
â€²
)=
â©
â¨
â§
	â€‹

+100,
âˆ’5,
âˆ’0.1,
	â€‹

s
â€²
=s
goal
	â€‹

s
â€²
=sÂ (invalidÂ move)
otherwise
	â€‹


This design rewards reaching the goal, penalizes collisions, and discourages idle or oscillatory movement.

Experimental Results
Maze Size	Iterations to Converge	Path Length	Discount Factor (
ğ›¾
Î³)
10Ã—10	538	471	0.995
20Ã—20	1834	775	0.995

The number of states grows quadratically with maze size (
âˆ£
ğ‘†
âˆ£
âˆ
ğ‘
2
âˆ£Sâˆ£âˆN
2
), leading to proportional increases in computational cost.
Value Iteration remains numerically stable and produces globally consistent trajectories even in larger environments.

Implementation Structure
Component	Description
MazeMDP	Environment definition (state validity, transitions, reward)
Value Iteration	Iterative Bellman updates and convergence detection
Policy Extraction	Derivation of optimal stateâ€“action mapping
Visualization	Static and animated rendering of the optimal trajectory
Visualization Example

Left: Original maze (walls = black, paths = white)

Right: Solved maze (optimal path = red, start = green, goal = blue)

maze1.png           # 10Ã—10 maze input
maze1-solved.png    # 10Ã—10 solution
maze.png            # 20Ã—20 maze input
maze-solved.png     # 20Ã—20 solution

How to Run
Dependencies
pip install numpy matplotlib opencv-python pillow

Execute

Run the notebook in Jupyter or VSCode:

jupyter notebook project1.ipynb


The output includes:

Value function convergence logs

Path length and iteration metrics

Visualized optimal solution (solvedmaze.png)
